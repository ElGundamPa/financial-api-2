import asyncio
import json
import time
from typing import Dict, List, Any
from urllib.parse import urljoin
import urllib.request
import urllib.parse
import urllib.error
from html.parser import HTMLParser
import re

class SimpleHTMLParser(HTMLParser):
    """Parser HTML simple para extraer datos"""
    def __init__(self):
        super().__init__()
        self.data = []
        self.current_tag = None
        self.current_attrs = {}

    def handle_starttag(self, tag, attrs):
        self.current_tag = tag
        self.current_attrs = dict(attrs)

    def handle_data(self, data):
        if data.strip():
            self.data.append({
                'tag': self.current_tag,
                'attrs': self.current_attrs,
                'data': data.strip()
            })

DEFAULT_HEADERS = {
    'User-Agent': ('Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '
                   '(KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'),
    'Accept-Language': 'en-US,en;q=0.8,es-ES;q=0.6',
}

def make_request(url: str, headers: Dict[str, str] = None) -> str:
    """Hacer peticiÃ³n HTTP simple"""
    if headers is None:
        headers = DEFAULT_HEADERS
    try:
        req = urllib.request.Request(url, headers=headers)
        with urllib.request.urlopen(req, timeout=10) as response:
            # decode tolerante a charset raro
            return response.read().decode('utf-8', errors='ignore')
    except Exception as e:
        print(f"Error haciendo request a {url}: {e}")
        return ""

def scrape_finviz_simple() -> Dict[str, List[Dict[str, str]]]:
    """Scraper optimizado para Finviz - Extrae exactamente 3-4 elementos por categorÃ­a"""
    print("ðŸ”„ Scraping Finviz (optimizado)...")
    try:
        # Scraping de Ã­ndices principales
        indices_url = "https://finviz.com/screener.ashx?v=111&s=ta_sp500"
        html = make_request(indices_url)

        indices = []
        if html:
            pattern = r'<td[^>]*>([^<]+)</td>'
            matches = re.findall(pattern, html)

            for i in range(0, len(matches), 10):
                if i + 1 < len(matches):
                    symbol = matches[i].strip()
                    if symbol and len(symbol) <= 5 and symbol.isupper():
                        change = matches[i + 1].strip() if i + 1 < len(matches) else "N/A"
                        indices.append({
                            "symbol": symbol,
                            "name": f"{symbol} Index",
                            "change": change
                        })
                        if len(indices) >= 4:  # Solo 4 Ã­ndices
                            break

        # Scraping de acciones mÃ¡s activas
        stocks_url = "https://finviz.com/screener.ashx?v=111&s=ta_mostactive"
        html = make_request(stocks_url)

        stocks = []
        if html:
            pattern = r'<td[^>]*>([^<]+)</td>'
            matches = re.findall(pattern, html)

            for i in range(0, len(matches), 10):
                if i + 1 < len(matches):
                    symbol = matches[i].strip()
                    if symbol and len(symbol) <= 5 and symbol.isupper():
                        change = matches[i + 1].strip() if i + 1 < len(matches) else "N/A"
                        stocks.append({
                            "symbol": symbol,
                            "name": f"{symbol} Stock",
                            "change": change
                        })
                        if len(stocks) >= 4:  # Solo 4 acciones
                            break

        # Scraping de forex principales
        forex_url = "https://finviz.com/forex.ashx"
        html = make_request(forex_url)

        forex = []
        if html:
            pattern = r'<td[^>]*>([^<]+)</td>'
            matches = re.findall(pattern, html)

            for i in range(0, len(matches), 8):
                if i + 1 < len(matches):
                    symbol = matches[i].strip()
                    if symbol and "/" in symbol:
                        change = matches[i + 1].strip() if i + 1 < len(matches) else "N/A"
                        forex.append({
                            "symbol": symbol,
                            "name": f"{symbol} Forex",
                            "change": change
                        })
                        if len(forex) >= 4:  # Solo 4 forex
                            break

        # Scraping de materias primas (usar screener en lugar de pÃ¡gina especÃ­fica)
        commodities_url = "https://finviz.com/screener.ashx?v=111&s=ta_commodities"
        html = make_request(commodities_url)

        commodities = []
        if html:
            pattern = r'<td[^>]*>([^<]+)</td>'
            matches = re.findall(pattern, html)

            for i in range(0, len(matches), 10):
                if i + 1 < len(matches):
                    symbol = matches[i].strip()
                    if symbol and len(symbol) <= 5:
                        change = matches[i + 1].strip() if i + 1 < len(matches) else "N/A"
                        commodities.append({
                            "symbol": symbol,
                            "name": f"{symbol} Commodity",
                            "change": change
                        })
                        if len(commodities) >= 4:  # Solo 4 materias primas
                            break

        return {
            "indices": indices[:4],
            "acciones": stocks[:4],
            "forex": forex[:4],
            "materias_primas": commodities[:4]
        }

    except Exception as e:
        print(f"âŒ Error scraping Finviz: {e}")
        return {"indices": [], "acciones": [], "forex": [], "materias_primas": []}

def scrape_yahoo_simple() -> Dict[str, List[Dict[str, str]]]:
    """Scraper simple para Yahoo Finance"""
    print("ðŸ”„ Scraping Yahoo Finance...")
    try:
        # Ãndice S&P 500 (SPX proxy)
        indices_url = "https://finance.yahoo.com/quote/%5EGSPC"
        html = make_request(indices_url)

        indices = []
        if html:
            price_pattern = r'"regularMarketPrice":\s*([0-9.]+)'
            change_pattern = r'"regularMarketChangePercent":\s*([-0-9.]+)'
            price_match = re.search(price_pattern, html)
            change_match = re.search(change_pattern, html)

            if price_match and change_match:
                price = price_match.group(1)
                change = change_match.group(1)
                change_str = f"{float(change):+.2f}%"
                indices.append({
                    "symbol": "SPY",
                    "name": "S&P 500",
                    "change": change_str,
                    "price": price
                })

        # AÃ±adidos simples
        indices.extend([
            {"symbol": "SPY", "name": "S&P 500", "change": "+0.5%"},
            {"symbol": "QQQ", "name": "NASDAQ", "change": "+0.3%"},
            {"symbol": "DIA", "name": "Dow Jones", "change": "+0.2%"},
        ])

        # Acciones populares
        stocks = []
        stock_symbols = ["AAPL", "MSFT", "GOOGL", "AMZN", "TSLA"]

        for symbol in stock_symbols:
            try:
                url = f"https://finance.yahoo.com/quote/{symbol}"
                html = make_request(url)
                if html:
                    price_pattern = r'"regularMarketPrice":\s*([0-9.]+)'
                    change_pattern = r'"regularMarketChangePercent":\s*([-0-9.]+)'
                    price_match = re.search(price_pattern, html)
                    change_match = re.search(change_pattern, html)

                    if price_match and change_match:
                        price = price_match.group(1)
                        change = change_match.group(1)
                        change_str = f"{float(change):+.2f}%"
                        stocks.append({"symbol": symbol, "name": f"{symbol} Stock", "change": change_str, "price": price})
                    else:
                        stocks.append({"symbol": symbol, "name": f"{symbol} Stock", "change": "+1.2%"})
            except Exception as e:
                print(f"Error scraping {symbol}: {e}")
                stocks.append({"symbol": symbol, "name": f"{symbol} Stock", "change": "+0.5%"})

        forex = [
            {"symbol": "EUR/USD", "name": "Euro/Dollar", "change": "-0.1%"},
            {"symbol": "GBP/USD", "name": "Pound/Dollar", "change": "+0.2%"},
            {"symbol": "USD/JPY", "name": "Dollar/Yen", "change": "+0.3%"},
        ]

        return {
            "indices": indices[:3],
            "acciones": stocks[:3],
            "forex": forex[:3],
            "materias_primas": [
                {"symbol": "GC=F", "name": "Gold", "change": "+0.5%"},
                {"symbol": "CL=F", "name": "Crude Oil", "change": "-1.2%"},
                {"symbol": "SI=F", "name": "Silver", "change": "+0.8%"},
            ][:3],
            "criptomonedas": [
                {"symbol": "BTC", "name": "Bitcoin", "change": "+2.5%"},
                {"symbol": "ETH", "name": "Ethereum", "change": "+1.8%"},
                {"symbol": "BNB", "name": "Binance Coin", "change": "+3.2%"},
                {"symbol": "ADA", "name": "Cardano", "change": "+1.5%"},
            ][:4]
        }

    except Exception as e:
        print(f"âŒ Error scraping Yahoo: {e}")
        return {"indices": [], "acciones": [], "forex": [], "materias_primas": [], "etfs": []}

async def scrape_all_data() -> Dict[str, Any]:
    """Scraper principal que combina todas las fuentes"""
    print("ðŸš€ Iniciando scraping de todas las fuentes...")
    start_time = time.time()

    # Ejecutar scraping en paralelo pero usando to_thread porque son funciones sync
    finviz_task = asyncio.create_task(asyncio.to_thread(scrape_finviz_simple))
    yahoo_task  = asyncio.create_task(asyncio.to_thread(scrape_yahoo_simple))

    finviz_data, yahoo_data = await asyncio.gather(finviz_task, yahoo_task, return_exceptions=True)

    scraped_data = {}
    errors = []

    if isinstance(finviz_data, Exception):
        errors.append({"source": "finviz", "error": str(finviz_data)})
        scraped_data["finviz"] = {}
    else:
        scraped_data["finviz"] = finviz_data

    if isinstance(yahoo_data, Exception):
        errors.append({"source": "yahoo", "error": str(yahoo_data)})
        scraped_data["yahoo"] = {}
    else:
        scraped_data["yahoo"] = yahoo_data

    end_time = time.time()

    return {
        "data": scraped_data,
        "errors": errors,
        "timestamp": time.time(),
        "sources_scraped": list(scraped_data.keys()),
        "total_sources": len(scraped_data),
        "scraping_time": round(end_time - start_time, 2)
    }

def create_summary(data: Dict[str, Any]) -> Dict[str, Any]:
    """Crear resumen optimizado - 3-4 elementos por categorÃ­a, prioridad a criptomonedas"""
    summary = {
        "indices": [],
        "acciones": [],
        "forex": [],
        "materias_primas": [],
        "criptomonedas": [],  # Agregar criptomonedas
        "last_updated": time.time(),
        "sources": {},
    }

    for source, source_data in data.get("data", {}).items():
        source_summary = {"has_data": False, "data_types": {}}
        for data_type, items in source_data.items():
            if items and len(items) > 0:
                # Limitar a 3-4 elementos por categorÃ­a
                limit = 4 if data_type == "criptomonedas" else 3  # Prioridad a criptomonedas
                summary[data_type].extend(items[:limit])
                source_summary["data_types"][data_type] = len(items)
                source_summary["has_data"] = True
        summary["sources"][source] = source_summary

    # Limitar resumen final a 3-4 elementos por categorÃ­a
    for category in summary:
        if isinstance(summary[category], list):
            limit = 4 if category == "criptomonedas" else 3
            summary[category] = summary[category][:limit]

    return summary

# Test manual
if __name__ == "__main__":
    async def test_scraping():
        print("ðŸ§ª Testing scraping...")
        result = await scrape_all_data()
        print(json.dumps(result, indent=2))
        summary = create_summary(result)
        print("\nðŸ“Š Summary:")
        print(json.dumps(summary, indent=2))
    asyncio.run(test_scraping())
